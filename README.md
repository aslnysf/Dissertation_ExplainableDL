# Explainable Deep Learning: A Study on Understanding Learning Process of Convolutional Neural Networks with Information Theory

  The field of artificial intelligence and machine learning have been developing rapidly throughout the years. The success, especially in deep learning during the last decade, has been fast with their unpredictable achievements in international challenges. Deep learning algorithms have made remarkable progress on numerous machine learning tasks and dramatically improved the state-of-the-art in many functional areas ranging from visual image recognition to understanding languages from audio (Graves et al., 2013; Zhang and LeCun, 2015; Hinton et al., 2012; He et al. 2015; LeCun et al., 2015). In the result of this success; deep learning models have been used in various application areas such as criminal justice, medicine and finance.
<br />
  Despite their great success, there is still no comprehensive understanding of the optimisation process or the internal organisation of deep neural networks, and they are often criticised for being used as mysterious ”black boxes” ( Alain and Bengio,2016; Adadi and Berrada, 2018). Deep learning models usually contain millions of parameters and functions. People can not understand this representation, and they also can not interpret the results of models. The lack of understanding can lead to a belief that the models are untrustworthy. Additionally, there is no way to know if the reasons behind the results are ill-formatted, biased or even wrong, which can raise many ethical, financial and legal issues. Studies on Explainable Machine
Learning try to provide solutions to this kind of problems. Explainable Machine Learning models are models that provide reasoning for the models’ results.
<br />
  Convolutional neural networks are highly demanded models for many computer vision tasks. Although it is used to solve many problems, the learning processes of convolutional neural networks are still not transparent. In recent years, many studies have been done to explain these models. However, the theoretical understanding of convolutional neural networks is still insufficient. With the motivation to make convolutional neural networks more intuitive, the aim of this thesis is stated as understanding the learning process of convolutional neural networks with information theory. This goal has been trying to achieve by answering two fundamental questions. The first research question for this dissertation was: What are the effects of hidden layers on learning of a convolutional neural network? and the second one was: How does
training affect the learning process of a convolutional neural network?.
<br />
  The research questions investigated and studied in the vision of information theory. This idea was theoretically introduced and proposed firstly on Tishby’s research papers on deep learning and information theory (Shwartz-Ziv and Tishby, 2017;Tishby and Zaslavsky, 2015 ). Later, some other researchers also utilise this theory to understanding deep neural networks (Balda,Behboodi and Mathar, 2018). In the context of this dissertation, the previously proposed method for analysing deep neural network was adopted to investigating the learning process of convolutional neural networks.
<br />
  In this dissertation, a systematic method proposed to analyse the learning process of a convolutional neural network. With answering the research question mentioned, the dissertation makes the following contributions:
      1) By calculating mutual information of individual layers, their effect on the learning process can be inferred from these quantities.
      2) By observing mutual information between input and output during the training, the effect of the training on the learning process of the model can be examined. The results show that there is a parallelism between the information-theoretic approach and the mathematical approach when determining optimum training.
